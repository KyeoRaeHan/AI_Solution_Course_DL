{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlp10_Text Mining.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP1i/FRN7V1l/6XU1XE06AH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KyeoRaeHan/AI_Solution_Course_DL/blob/main/nlp10_Text_Mining.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-vBd-msyLjEe"
      },
      "source": [
        "## 텍스트 분석 수행 프로세스\n",
        "\n",
        "# 1. 텍스트 사전 준비작업(텍스트 전처리): 텍스트를 피처로 만들기 전에 미리 클렌징. 대/소문자 변경, 특수문자 삭제 등의 클렌징 작업, 단어(Word) 등의 토큰화 작업. 의미없는 단어(Stop Word) 제거 작업, 어근 추출(Stemming/Lemmatization) 등의 텍스트 정규화 작업을 수행하는 것을 통칭합니다.\n",
        "\n",
        "2. 피처 벡터화/추출: 사전 준비 작업으로 가공된 텍스트에서 피처를 추출하고 여기에 벡터값을 할당합니다. 대표적인 방법은 BOW와 Word2Vec이 있으며, BOW는 대표적으로 Count기반 TF-IDF 기반 벡터화가 있습니다.\n",
        "\n",
        "3. ML 모델 수립 및 학습/예측/평가: 피처 벡터화된 데이터 세트에 ML모델을 적용해 학습/예측 및 평가를 수행합니다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBEPLrlE1lEf"
      },
      "source": [
        "## NLP, 텍스트 분석\n",
        "- Natural Language Processing : 기계가 인간의 언어를 이해하고 해석하는데 중점. 기계번역, 질의응답시스템\n",
        "- 텍스트 분석 : 비정형 텍스트에서 의미있는 정보를 추출하는 것에 중점\n",
        "- NLP는 텍스트 분석을 향상하게 하는 기반 기술\n",
        "- NLP와 텍스트 분석의 근간에는 머신러닝이 존재. 과거 언어적인 룰 기반 시스템에서 텍스트 데이터 기반으로 모델을 학습하고 예측\n",
        "- 텍스트 분석은 머신러닝, 언어 이해, 통계 등을 활용한 모델 수립, 정보 추출을 통해 인사이트 및 예측 분석 등의 분석 작업 수행\n",
        "  - 텍스트 분류 : 신문기사 카테고리 분류, 스팸 메일 검출 프로그램. 지도학습\n",
        "  - 감성 분석 : 감정/판단/믿음/의견/기분 등의 주관적 요소 분석. 소셜미디어 감정분석, 영화 리뷰, 여론조사 의견분석. 지도학습, 비지도학습\n",
        "  - 텍스트 요약 : 텍스트 내에서 중요한 주제나 중심 사상을 추출. 토픽 모델링\n",
        "  - 텍스트 군집화와 유사도 측정 : 비슷한 유형의 문서에 대해 군집화 수행. 비지도 학습\n",
        "\n",
        "## Text 분석 수행 프로세스\n",
        "- 텍스트 정규화\n",
        "  - 클랜징, 토큰화, 필터링/스톱워드 제거/철자 수정, Stemming, Lemmatization\n",
        "- 피처 벡터화 변환\n",
        "  - Bag of Words : Count 기반, TF-IDF 기반\n",
        "  - Word2Vec\n",
        "- ML 모델 수립 및 학습/예측/평가\n",
        "\n",
        "## 텍스트 전처리 - 텍스트 정규화\n",
        "- 클렌징 : 분석에 방해되는 불필요한 문자, 기호를 사전에 제거. HTML, XML 태그나 특정 기호\n",
        "- 토큰화 : 문서에서 문장을 분리하는 문장 토큰화와 문장에서 단어를 토큰으로 분리하는 단어 토큰화\n",
        "- 필터링/스톱워드 제거/철자 수정 : 분석에 큰 의미가 없는 단어를 제거\n",
        "- Stemming, Lemmatization : 문법적 또는 의미적으로 변화하는 단어의 원형을 찾음\n",
        "  - Stemming은 원형 단어로 변환 시 일반적인 방법을 적용하거나 더 단순화된 방법을 적용\n",
        "  - Lemmatization이 Stemming 보다 정교하며 의미론적인 기반에서 단어의 원형을 찾음\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdStNCiHK2W9"
      },
      "source": [
        "# 텍스트 분석 패키지: NLTK, Gensim, SpacCy\n",
        "## Gensim, SpacCy은 실제 업무에서 자주 활용되는 패키지"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6SItMNV6rZA"
      },
      "source": [
        "# 토큰화를 위한 API를 제공 토큰화를 위한 API를 제공\n",
        "import nltk\n",
        "import nltk"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TECtFqi4DSwj",
        "outputId": "3c3fb01f-2d38-47c9-c324-d607d9cbf64a"
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwhTOiYlDcp3"
      },
      "source": [
        "from nltk import sent_tokenize"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y0yIC1BWDnIR",
        "outputId": "4564421b-c298-4afd-aae1-99c398ad1ec0"
      },
      "source": [
        "# 문장 토큰화: 문장의 마침표(.), 개행문자(\\n) 등 문장의 마지막으 뜻하는 기호에 따라 분리하는 것이 일반적.\n",
        "text_sample = 'In mathematics, a matrix (plural matrices) is. \\\n",
        "a rectangular array or table of numbers, symbols, or expression. \\\n",
        "arranged in rows and columns.'\n",
        "\n",
        "sentences = sent_tokenize(text=text_sample)\n",
        "print(type(sentences), len(sentences))\n",
        "print(sentences)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'list'> 3\n",
            "['In mathematics, a matrix (plural matrices) is.', 'a rectangular array or table of numbers, symbols, or expression.', 'arranged in rows and columns.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8LHiNnJOR-g"
      },
      "source": [
        "# sent_tokenize()를 사용하여 3개의 문장이 3개의 요소를 포함한 list형으로 변환되었다."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8oFnJplMD6U0",
        "outputId": "1d17e578-852c-4352-f869-ab52d0610e67"
      },
      "source": [
        "# 단어 토근화(word_tokenize): 공백, 콤마(,), 마침표(.), 개행문자, 정규표현식\n",
        "from nltk import word_tokenize\n",
        "\n",
        "sentence = 'In mathematics, a matrix (plural matrices) is a rectangular array or table of numbers, symbols, or expression.'\n",
        "words = word_tokenize(sentence)\n",
        "print(words)\n",
        "print(type(words), len(words))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['In', 'mathematics', ',', 'a', 'matrix', '(', 'plural', 'matrices', ')', 'is', 'a', 'rectangular', 'array', 'or', 'table', 'of', 'numbers', ',', 'symbols', ',', 'or', 'expression', '.']\n",
            "<class 'list'> 23\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4cnLK4bEEs12",
        "outputId": "ff0c37df-e294-4f92-9265-d02048ce99af"
      },
      "source": [
        "# 문서에 대해서 모든 단어를 토큰화 문서에 대해서 모든 단어를 토큰화\n",
        "from nltk import word_tokenize, sent_tokenize\n",
        "\n",
        "# 여러 개의 문장으로 된 입력 데이터를 문장별로 단어 토큰화하게 만드는 함수 생성\n",
        "def tokenize_text(text):\n",
        "  sentences = sent_tokenize(text) #문장별 분리 토큰\n",
        "  word_tokens = [word_tokenize(sentence) for sentence in sentences] # 문장별 단어 토큰화\n",
        "  return word_tokens\n",
        "\n",
        "# 여러 문장에 대해 문장별 단어 토큰화 수행.\n",
        "word_tokens = tokenize_text(text_sample)\n",
        "print(word_tokens)\n",
        "print(type(word_tokens), len(word_tokens)) "
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['In', 'mathematics', ',', 'a', 'matrix', '(', 'plural', 'matrices', ')', 'is', '.'], ['a', 'rectangular', 'array', 'or', 'table', 'of', 'numbers', ',', 'symbols', ',', 'or', 'expression', '.'], ['arranged', 'in', 'rows', 'and', 'columns', '.']]\n",
            "<class 'list'> 3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUJrH6L2QzLW"
      },
      "source": [
        "# 3개 문장을 먼저 토큰화했으므로 3개의 리스트 객체를 내포하는 리스트, \n",
        "# 그리고 내포된 개별 리스트 객체는 각각 문장별로 토큰화된 단어 요소를 가지고 있다."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1GfpK3aHG-OZ",
        "outputId": "dd7b3a5b-b14d-473a-fb39-ddab6e56d368"
      },
      "source": [
        "# 스톱워드 제거: the, is, a, will와 같이 '문맥적'으로 큰 의미가 없는 단어를 제거\n",
        "# 위 단어들은 문장에서 빈번하게 등장해서, 모델이 위 단어들을 중요한 데이터로 학습할 수 있으므로 \n",
        "# 전처리시 스톱워드를 제거하는 것이 매우 중요하다.\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r9iOgB0cIBHL",
        "outputId": "92176845-9ee1-4d6f-c4d8-0506d5c9da0c"
      },
      "source": [
        "# NLTK english stopwords 갯수 확인\n",
        "# corpus는 형태소 NLTK english stopwords 갯수 확인\n",
        "# corpus는 형태소\n",
        "print(len(nltk.corpus.stopwords.words('english')))\n",
        "print(nltk.corpus.stopwords.words('english')[:20])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "179\n",
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4K7q21EbIjeV",
        "outputId": "6937b0b1-8f01-4d8a-8bb1-c186428f2322"
      },
      "source": [
        "# stopwords 필터링을 통한 제거\n",
        "import nltk\n",
        "\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "all_tokens = []\n",
        "# 위 예제에서 3개의 문장별로 얻은 word_tokens list에 대해 스톱 워드를 제거하는 반복문\n",
        "for sentence in word_tokens:\n",
        "  filtered_words = []\n",
        "# 개별 문장별로 토큰화된 문장 list에 대해 스톱 워드를 제거하는 반복문\n",
        "  for word in sentence:\n",
        "    # 소문자로 반환\n",
        "    word = word.lower()\n",
        "    # 토큰화된 개별 단어가 스톱 워드의 단어에 포함되지 않으면 word_tokens에 추가\n",
        "    if word not in stopwords:\n",
        "      filtered_words.append(word)\n",
        "  all_tokens.append(filtered_words)\n",
        "print(all_tokens)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['mathematics', ',', 'matrix', '(', 'plural', 'matrices', ')', '.'], ['rectangular', 'array', 'table', 'numbers', ',', 'symbols', ',', 'expression', '.'], ['arranged', 'rows', 'columns', '.']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cvQlyud-KIi7",
        "outputId": "ef77df63-2f68-499c-b58f-0f5986638b8e"
      },
      "source": [
        "# 문법적 또는 의미적으로 변화하는 '단어의 원형'을 찾는 방법\n",
        "# stemmer(LancasterStemmer)\n",
        "from nltk.stem import LancasterStemmer\n",
        "stemmer = LancasterStemmer()\n",
        "print(stemmer.stem('working'), stemmer.stem('works'), stemmer.stem('worked'))\n",
        "print(stemmer.stem('amusing'), stemmer.stem('amuses'), stemmer.stem('amused'))\n",
        "print(stemmer.stem('fancier'), stemmer.stem('fanciest'))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "work work work\n",
            "amus amus amus\n",
            "fant fanciest\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8O9ZWlDLF0r"
      },
      "source": [
        "# stemming은 단순히 원래 단어에서 일부 철자가 훼손된 어근 단어를 추출하는 경향이 있어,\n",
        "# amus, fant처럼 결과가 이상하게 나온다. 그래서 여기서 더 발전하여 나온게 'WordNet'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mLtaLyd-NE-r",
        "outputId": "0786566c-0d6e-43de-93d5-b792704e80cf"
      },
      "source": [
        "nltk.download('wordnet')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j86Ap8r5LT9V",
        "outputId": "35181d66-892a-40b1-989c-cf320f816ede"
      },
      "source": [
        "# Lemmatization(WordNetLemmatizer): 보다 정확한 원형 단어 추출을 위해 단어의 '품사'를 직접 입력\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "\n",
        "lemma = WordNetLemmatizer()\n",
        "print(lemma.lemmatize('working', 'v'),lemma.lemmatize('works', 'v'),lemma.lemmatize('worked', 'v'))\n",
        "print(lemma.lemmatize('amusing', 'v'),lemma.lemmatize('amuses', 'v'), lemma.lemmatize('amused', 'v'))\n",
        "print(lemma.lemmatize('happier', 'a'),lemma.lemmatize('happiest', 'a'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "work work work\n",
            "amuse amuse amuse\n",
            "happy happy\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17PGyTt0L4_l"
      },
      "source": [
        "# GPU VS CPU"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hc6O7eruC7U4",
        "outputId": "d5a2c648-2895-4250-c19f-c9df8a697780"
      },
      "source": [
        "import numpy as np\n",
        "num_samples = 100\n",
        "height = 71\n",
        "width = 71\n",
        "num_classes = 100\n",
        "\n",
        "import tensorflow as tf\n",
        "from keras.applications import Xception\n",
        "import datetime\n",
        "start = datetime.datetime.now()\n",
        "\n",
        "model = Xception(weights = None,\n",
        "                 input_shape=(height,width,3),\n",
        "                 classes=num_classes)\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='rmsprop')\n",
        "x=np.random.random((num_samples,height,width,3))\n",
        "y=np.random.random((num_samples,num_classes))\n",
        "\n",
        "model.fit(x,y,epochs=3,batch_size=16)\n",
        "model.save('my_model.h5')\n",
        "end = datetime.datetime.now()\n",
        "time_delta = end - start"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "7/7 [==============================] - 25s 2s/step - loss: 236.1511\n",
            "Epoch 2/3\n",
            "7/7 [==============================] - 15s 2s/step - loss: 244.4009\n",
            "Epoch 3/3\n",
            "7/7 [==============================] - 15s 2s/step - loss: 246.2726\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gP6bVCBsopr9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}