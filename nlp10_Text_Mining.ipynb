{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlp10_Text Mining.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPZg7B0BjrNjG1jzmDB+Ka3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KyeoRaeHan/AI_Solution_Course_DL/blob/main/nlp10_Text_Mining.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-vBd-msyLjEe"
      },
      "source": [
        "## 텍스트 분석 수행 프로세스\n",
        "\n",
        "1. 텍스트 사전 준비작업(텍스트 전처리): 텍스트를 피처로 만들기 전에 미리 클렌징. 대/소문자 변경, 특수문자 삭제 등의 클렌징 작업, 단어(Word) 등의 토큰화 작업. 의미없는 단어(Stop Word) 제거 작업, 어근 추출(Stemming/Lemmatization) 등의 텍스트 정규화 작업을 수행하는 것을 통칭합니다.\n",
        "\n",
        "2. 피처 벡터화/추출: 사전 준비 작업으로 가공된 텍스트에서 피처를 추출하고 여기에 벡터값을 할당합니다. 대표적인 방법은 BOW와 Word2Vec이 있으며, BOW는 대표적으로 Count기반 TF-IDF 기반 벡터화가 있습니다.\n",
        "\n",
        "3. ML 모델 수립 및 학습/예측/평가: 피처 벡터화된 데이터 세트에 ML모델을 적용해 학습/예측 및 평가를 수행합니다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdStNCiHK2W9"
      },
      "source": [
        "# 텍스트 분석 패키지: NLTK, Gensim, SpacCy\n",
        "## Gensim, SpacCy은 실제 업무에서 자주 활용되는 패키지"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6SItMNV6rZA"
      },
      "source": [
        "import nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TECtFqi4DSwj",
        "outputId": "5b5b99bf-3c5e-4453-b05b-9884b630e52c"
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwhTOiYlDcp3"
      },
      "source": [
        "from nltk import sent_tokenize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y0yIC1BWDnIR",
        "outputId": "16a86134-90ee-48ff-b286-45b6f51d6b42"
      },
      "source": [
        "# 문장 토큰화: 문장의 마침표(.), 개행문자(\\n) 등 문장의 마지막으 뜻하는 기호에 따라 분리하는 것이 일반적.\n",
        "text_sample = 'In mathematics, a matrix (plural matrices) is. \\\n",
        "a rectangular array or table of numbers, symbols, or expression. \\\n",
        "arranged in rows and columns.'\n",
        "\n",
        "sentences = sent_tokenize(text=text_sample)\n",
        "print(type(sentences), len(sentences))\n",
        "print(sentences)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'list'> 3\n",
            "['In mathematics, a matrix (plural matrices) is.', 'a rectangular array or table of numbers, symbols, or expression.', 'arranged in rows and columns.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8LHiNnJOR-g"
      },
      "source": [
        "# sent_tokenize()를 사용하여 3개의 문장이 3개의 요소를 포함한 list형으로 변환되었다."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8oFnJplMD6U0",
        "outputId": "709fb083-55f5-4fd0-d759-6b3dd9268643"
      },
      "source": [
        "# 단어 토근화(word_tokenize): 공백, 콤마(,), 마침표(.), 개행문자, 정규표현식\n",
        "from nltk import word_tokenize\n",
        "\n",
        "sentence = 'In mathematics, a matrix (plural matrices) is a rectangular array or table of numbers, symbols, or expression.'\n",
        "words = word_tokenize(sentence)\n",
        "print(words)\n",
        "print(type(words), len(words))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['In', 'mathematics', ',', 'a', 'matrix', '(', 'plural', 'matrices', ')', 'is', 'a', 'rectangular', 'array', 'or', 'table', 'of', 'numbers', ',', 'symbols', ',', 'or', 'expression', '.']\n",
            "<class 'list'> 23\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4cnLK4bEEs12",
        "outputId": "e209cec6-47fa-45eb-9fa7-7e95bf777dd8"
      },
      "source": [
        "from nltk import word_tokenize, sent_tokenize\n",
        "\n",
        "# 여러 개의 문장으로 된 입력 데이터를 문장별로 단어 토큰화하게 만드는 함수 생성\n",
        "def tokenize_text(text):\n",
        "  sentences = sent_tokenize(text) #문장별 분리 토큰\n",
        "  word_tokens = [word_tokenize(sentence) for sentence in sentences] # 문장별 단어 토큰화\n",
        "  return word_tokens\n",
        "\n",
        "# 여러 문장에 대해 문장별 단어 토큰화 수행.\n",
        "word_tokens = tokenize_text(text_sample)\n",
        "print(word_tokens)\n",
        "print(type(word_tokens), len(word_tokens)) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['In', 'mathematics', ',', 'a', 'matrix', '(', 'plural', 'matrices', ')', 'is', '.'], ['a', 'rectangular', 'array', 'or', 'table', 'of', 'numbers', ',', 'symbols', ',', 'or', 'expression', '.'], ['arranged', 'in', 'rows', 'and', 'columns', '.']]\n",
            "<class 'list'> 3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUJrH6L2QzLW"
      },
      "source": [
        "# 3개 문장을 먼저 토큰화했으므로 3개의 리스트 객체를 내포하는 리스트, 그리고 내포된 개별 리스트 객체는 각각 문장별로 토큰화된 단어 요소를 가지고 있다."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1GfpK3aHG-OZ",
        "outputId": "18445484-4953-4053-9078-9d8380a92784"
      },
      "source": [
        "# 스톱워드 제거: the, is, a, will와 같이 '문맥적'으로 큰 의미가 없는 단어를 제거\n",
        "# 위 단어들은 문장에서 빈번하게 등장해서, 모델이 위 단어들을 중요한 데이터로 학습할 수 있으므로 전처리시 스톱워드를 제거하는 것이 매우 중요하다.\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r9iOgB0cIBHL",
        "outputId": "19aec9c3-b09a-4b8c-db4f-a3f1d9fe2905"
      },
      "source": [
        "# NLTK english stopwords 갯수 확인\n",
        "print(len(nltk.corpus.stopwords.words('english')))\n",
        "print(nltk.corpus.stopwords.words('english')[:20])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "179\n",
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4K7q21EbIjeV",
        "outputId": "04ae9b79-4ff6-4647-e38e-4603fa5551d8"
      },
      "source": [
        "# stopwords 필터링을 통한 제거\n",
        "import nltk\n",
        "\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "all_tokens = []\n",
        "# 위 예제에서 3개의 문장별로 얻은 word_tokens list에 대해 스톱 워드를 제거하는 반복문\n",
        "for sentence in word_tokens:\n",
        "  filtered_words = []\n",
        "# 개별 문장별로 토큰화된 문장 list에 대해 스톱 워드를 제거하는 반복문\n",
        "  for word in sentence:\n",
        "    # 소문자로 반환\n",
        "    word = word.lower()\n",
        "    # 토큰화된 개별 단어가 스톱 워드의 단어에 포함되지 않으면 word_tokens에 추가\n",
        "    if word not in stopwords:\n",
        "      filtered_words.append(word)\n",
        "  all_tokens.append(filtered_words)\n",
        "print(all_tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['mathematics', ',', 'matrix', '(', 'plural', 'matrices', ')', '.'], ['rectangular', 'array', 'table', 'numbers', ',', 'symbols', ',', 'expression', '.'], ['arranged', 'rows', 'columns', '.']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cvQlyud-KIi7",
        "outputId": "0eab4847-433e-46e6-b422-4d9f0e6690e2"
      },
      "source": [
        "# 문법적 또는 의미적으로 변화하는 단어의 원형을 찾는 방법\n",
        "# stemmer(LancasterStemmer)\n",
        "from nltk.stem import LancasterStemmer\n",
        "stemmer = LancasterStemmer()\n",
        "print(stemmer.stem('working'), stemmer.stem('works'), stemmer.stem('worked'))\n",
        "print(stemmer.stem('amusing'), stemmer.stem('amuses'), stemmer.stem('amused'))\n",
        "print(stemmer.stem('fancier'), stemmer.stem('fanciest'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "work work work\n",
            "amus amus amus\n",
            "fant fanciest\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8O9ZWlDLF0r"
      },
      "source": [
        "# stemming은 단순히 원래 단어에서 일부 철자가 훼손된 어근 단어를 추출하는 경햐이 있어,\n",
        "# amus, fant처럼 결과가 이상하게 나온다. 그래서 여기서 더 발전하여 나온게 'WordNet'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mLtaLyd-NE-r",
        "outputId": "dd8dfbf6-e831-445b-c57b-060f10261ed3"
      },
      "source": [
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j86Ap8r5LT9V",
        "outputId": "35181d66-892a-40b1-989c-cf320f816ede"
      },
      "source": [
        "# Lemmatization(WordNetLemmatizer): 보다 정확한 원형 단어 추출을 위해 단어의 '품사'를 직접 입력\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "\n",
        "lemma = WordNetLemmatizer()\n",
        "print(lemma.lemmatize('working', 'v'),lemma.lemmatize('works', 'v'),lemma.lemmatize('worked', 'v'))\n",
        "print(lemma.lemmatize('amusing', 'v'),lemma.lemmatize('amuses', 'v'), lemma.lemmatize('amused', 'v'))\n",
        "print(lemma.lemmatize('happier', 'a'),lemma.lemmatize('happiest', 'a'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "work work work\n",
            "amuse amuse amuse\n",
            "happy happy\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17PGyTt0L4_l"
      },
      "source": [
        "# GPU VS CPU"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WB800PkYl4l6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 239
        },
        "outputId": "b59077cd-f6b0-402a-c0d5-9562a582cb40"
      },
      "source": [
        "import numpy as np\n",
        "num_samples = 100\n",
        "height = 71\n",
        "width = 71\n",
        "num_classes = 100\n",
        "\n",
        "import tensorflow as tf\n",
        "from keras.applications import Xception\n",
        "import datetime\n",
        "start = datetime.datetime.now()\n",
        "\n",
        "model = Xception(weight = None, input_shape=height, width=3, classes=num_classes)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
        "\n",
        "x = np.random((num_samples, height, width, 3))\n",
        "y = np.random((num_samples, num_classes))\n",
        "\n",
        "model.fit(x, y, epochs=3, batch_size=16)\n",
        "model.save('my_model.h5')\n",
        "end = datetime.datetime.now()\n",
        "time_delta = end - start"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-32fc9a124910>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mXception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rmsprop'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Xception() got an unexpected keyword argument 'weight'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gP6bVCBsopr9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}