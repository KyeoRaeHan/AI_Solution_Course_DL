{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "피처 벡터화 : One-hot encoding \n",
    "### Bag of Words \n",
    "- 문맥이나 순서를 무시하고 일괄적으로 단어에 대한 빈도 값을 부여해 피처 값을 추출하는 모델.\n",
    "- 문맥 의미 반영 부족, 희소 행렬 문제.\n",
    "### BOW에서 피처 벡터화 \n",
    "- 모든 단어를 컬럼 형태로 나열하고 각 문서에서 해당 단어의 횟수나 정규화된 빈도를 값으로 부여하는 데이터 세트 모델로 변경하는 것.\n",
    "### 피처 벡터화 방식 \n",
    "- 카운트 기반, TF-IDF(Term Frequency - Inverse Document Frequency) 기반 벡터화\n",
    "카운트 벡터화: 카운트 값이 높을수록 중요한 단어로 인식. 특성상 자주 사용되는 보편적인 단어까지 높은 값 부여\n",
    "TF-IDF: 모든 문서에서 전반적으로 자주 나타나는 단어에 대해서 패널티 부여. '빈번하게', '당연히', '조직', '업무' 등\n",
    "파라미터"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- max_df : 너무 높은 빈도수를 가지는 단어 피처를 제외\n",
    "- min_df : 너무 낮은 빈도수를 가지는 단어 피처를 제외\n",
    "- max_features : 추출하는 피처의 개수를 제한하며 정수로 값을 지정\n",
    "- stop_words : 'english'로 지정하면 스톱 워드로 지정된 단어는 추출에서 제외\n",
    "- n_gram_range : 튜플 형태로 (범위 최솟값, 범위 최댓값)을 지정\n",
    "- analyzer : 피처 추출을 수행하는 단위. 디폴트는 'word'\n",
    "- token_pattern : 토큰화를 수행하는 정규 표현식 패턴을 지정\n",
    "- tokenizer : 토큰화를 별도의 커스텀 함수로 이용시 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 피처벡터화\n",
    "# 공간이 넓어지면 희소해진다.. 희소행렬.. 그래서 PCA???\n",
    "# 정규표현식 -> 점프 투 파이썬 참고\n",
    "# 자연어분야를 할 사람들은 정규표현식을 인텐시브하게 할 필요가 있다. \n",
    "# 자연어 데이터 전처리에 정규표현식을 많이 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3, 0, 1],\n",
       "       [0, 2, 0]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# naddray 객체 생성\n",
    "import numpy as np\n",
    "dense = np.array([[3,0,1], [0,2,0]])\n",
    "dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t3\n",
      "  (0, 2)\t1\n",
      "  (1, 1)\t2\n"
     ]
    }
   ],
   "source": [
    "# 희소행렬 - COO 형식\n",
    "from scipy import sparse\n",
    "data = np.array([3,1,2]) # 0이 아닌 데이터\n",
    "row_pos = np.array([0,0,1]) #0이 아닌 데이터의 row 좌표\n",
    "col_pos = np.array([0,2,1]) #0이 아닌 데이터의 col 좌표\n",
    "sparse_coo = sparse.coo_matrix((data, (row_pos, col_pos))) \n",
    "# coo_matrix에 0이 아닌 데이터와 좌표를 인자로 넣어준다. coo형식으로 반환해준다.\n",
    "print(sparse_coo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2)\t1\n",
      "  (0, 5)\t5\n",
      "  (1, 0)\t1\n",
      "  (1, 1)\t4\n",
      "  (1, 3)\t3\n",
      "  (1, 4)\t2\n",
      "  (1, 5)\t5\n",
      "  (2, 1)\t6\n",
      "  (2, 3)\t3\n",
      "  (3, 0)\t2\n",
      "  (4, 3)\t7\n",
      "  (4, 5)\t8\n",
      "  (5, 0)\t1\n",
      "[[0 0 1 0 0 5]\n",
      " [1 4 0 3 2 5]\n",
      " [0 6 0 3 0 0]\n",
      " [2 0 0 0 0 0]\n",
      " [0 0 0 7 0 8]\n",
      " [1 0 0 0 0 0]]\n",
      "\n",
      "  (0, 2)\t1\n",
      "  (0, 5)\t5\n",
      "  (1, 0)\t1\n",
      "  (1, 1)\t4\n",
      "  (1, 3)\t3\n",
      "  (1, 4)\t2\n",
      "  (1, 5)\t5\n",
      "  (2, 1)\t6\n",
      "  (2, 3)\t3\n",
      "  (3, 0)\t2\n",
      "  (4, 3)\t7\n",
      "  (4, 5)\t8\n",
      "  (5, 0)\t1\n",
      "[[0 0 1 0 0 5]\n",
      " [1 4 0 3 2 5]\n",
      " [0 6 0 3 0 0]\n",
      " [2 0 0 0 0 0]\n",
      " [0 0 0 7 0 8]\n",
      " [1 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# 희소행렬 - COO vs CSR 형식\n",
    "dense2 = np.array([[0,0,1,0,0,5],\n",
    "                  [1,4,0,3,2,5],\n",
    "                  [0,6,0,3,0,0],\n",
    "                  [2,0,0,0,0,0],\n",
    "                  [0,0,0,7,0,8],\n",
    "                  [1,0,0,0,0,0]])\n",
    "data2 = np.array([1,5,1,4,3,2,5,6,3,2,7,8,1])\n",
    "row_pos = np.array([0, 0, 1, 1, 1, 1, 1, 2, 2, 3, 4, 4, 5])\n",
    "col_pos = np.array([2, 5, 0, 1, 3, 4, 5, 1, 3, 0, 3, 5, 0])\n",
    "# COO 형식으로 변환\n",
    "sparse_coo = sparse.coo_matrix((data2,(row_pos,col_pos)))\n",
    "print(sparse_coo)\n",
    "print(sparse_coo.toarray())\n",
    "print()\n",
    "# CSR 형식으로 변환\n",
    "# 행 위치 배열의 고유한 값들의 시작 위치 인덱스를 배열로 생성\n",
    "row_pos_ind = np.array([0,2,7,9,10,12,13])\n",
    "sparse_csr = sparse.csr_matrix((data2,col_pos,row_pos_ind))\n",
    "print(sparse_csr)\n",
    "print(sparse_csr.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 희소행렬 - CSR 형식\n",
    "dense2 = np.array([[0,0,1,0,0,5], [1,4,0,3,2,5], [0,6,0,3,0,0], \\\n",
    "                   [2,0,0,0,0,0], [0,0,0,7,0,8], [1,0,0,0,0,0]])\n",
    "\n",
    "data2 = np.array([1,5,1,4,3,2,5,6,3,2,7,8,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 6)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (1, 3)\t1\n",
      "  (1, 6)\t5\n",
      "  (2, 1)\t1\n",
      "  (2, 2)\t4\n",
      "  (2, 4)\t3\n",
      "  (2, 5)\t2\n",
      "  (2, 6)\t5\n",
      "  (3, 2)\t6\n",
      "  (3, 4)\t3\n",
      "  (4, 1)\t2\n",
      "  (5, 4)\t7\n",
      "  (5, 6)\t8\n",
      "  (6, 1)\t1\n",
      "[[0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 5]\n",
      " [0 1 4 0 3 2 5]\n",
      " [0 0 6 0 3 0 0]\n",
      " [0 2 0 0 0 0 0]\n",
      " [0 0 0 0 7 0 8]\n",
      " [0 1 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# COO 형식으로 변환\n",
    "row_pos = np.array([1,1,2,2,2,2,2,3,3,4,5,5,6])\n",
    "col_pos = np.array([3,6,1,2,4,5,6,2,4,1,4,6,1])\n",
    "sparse_coo = sparse.coo_matrix((data2, (row_pos, col_pos)))\n",
    "print(sparse_coo)\n",
    "print(sparse_coo.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 3)\t1\n",
      "  (0, 6)\t5\n",
      "  (1, 1)\t1\n",
      "  (1, 2)\t4\n",
      "  (1, 4)\t3\n",
      "  (1, 5)\t2\n",
      "  (1, 6)\t5\n",
      "  (2, 2)\t6\n",
      "  (2, 4)\t3\n",
      "  (3, 1)\t2\n",
      "  (4, 4)\t7\n",
      "  (4, 6)\t8\n",
      "  (5, 1)\t1\n",
      "[[0 0 0 1 0 0 5]\n",
      " [0 1 4 0 3 2 5]\n",
      " [0 0 6 0 3 0 0]\n",
      " [0 2 0 0 0 0 0]\n",
      " [0 0 0 0 7 0 8]\n",
      " [0 1 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# CSR 형식으로 변환\n",
    "# 행 위치 배열(row_pos)의 고유한 값들의 시작 위치 인덱스를 배열로 생성\n",
    "row_pos_ind = np.array([0,2,7,9,10,12,13]) #13은 전체 갯수를 의미\n",
    "sparse_csr = sparse.csr_matrix((data2, col_pos, row_pos_ind))\n",
    "print(sparse_csr)\n",
    "print(sparse_csr.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 2. 0.]\n",
      " [0. 3. 1.]]\n",
      "['A', 'B', 'C']\n",
      "{'A': 0, 'B': 1, 'C': 2}\n",
      "[[0. 0. 4.]]\n"
     ]
    }
   ],
   "source": [
    "# DicVectorizer : 문서에서 단어의 사용빈도를 나타내는 딕셔너리 정보를 입력받아 BOW 인코딩한 수치 벡터로 전환\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "v = DictVectorizer(sparse=False)\n",
    "D = [{'A':1, 'B':2}, {'B':3, 'C':1}]\n",
    "X = v.fit_transform(D)\n",
    "print(X)\n",
    "print(v.feature_names_) #피처의 이름\n",
    "print(v.vocabulary_) # 피처의 순서\n",
    "#사전에 D가 없기때문에 D가 출력이 안된다\n",
    "# D가 나오게 하려면 다시 D를 집어넣고 fit_transform을 해줘야 한다.\n",
    "print(v.transform({'C':4, 'D':3})) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This is the second document',\n",
    "    'Ant the third one.',\n",
    "    'Is this the first document?',\n",
    "    'The last document?'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ant', 'document', 'first', 'is', 'last', 'one', 'second', 'the', 'third', 'this']\n",
      "\n",
      "{'this': 9, 'is': 3, 'the': 7, 'first': 2, 'document': 1, 'second': 6, 'ant': 0, 'third': 8, 'one': 5, 'last': 4}\n"
     ]
    }
   ],
   "source": [
    "vect = CountVectorizer()\n",
    "vect.fit(corpus)\n",
    "print(vect.get_feature_names())\n",
    "print()\n",
    "# 알파벳 순으로 숫자 매겨줌\n",
    "print(vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 1 0 0 1 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "print(vect.transform(['This is the second document']).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(vect.transform(['Something completely new.']).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 1 0 0 0 1 0 1]\n",
      " [0 1 0 1 0 0 1 1 0 1]\n",
      " [1 0 0 0 0 1 0 1 1 0]\n",
      " [0 1 1 1 0 0 0 1 0 1]\n",
      " [0 1 0 0 1 0 0 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(vect.transform(corpus).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'first': 2,\n",
       " 'document': 1,\n",
       " 'second': 5,\n",
       " 'ant': 0,\n",
       " 'third': 6,\n",
       " 'one': 4,\n",
       " 'last': 3}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stop Words는 문서에서 단어장을 생성할 때 무시할 수 있는 단어.\n",
    "# 보통 영어의 관사, 접속사 한국어의 조사 등\n",
    "\n",
    "vect = CountVectorizer(stop_words=['and', 'is', 'the', 'this']).fit(corpus)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'document': 1, 'second': 2, 'ant': 0}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(stop_words='english').fit(corpus)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'t': 16,\n",
       " 'h': 8,\n",
       " 'i': 9,\n",
       " 's': 15,\n",
       " ' ': 0,\n",
       " 'e': 6,\n",
       " 'f': 7,\n",
       " 'r': 14,\n",
       " 'd': 5,\n",
       " 'o': 13,\n",
       " 'c': 4,\n",
       " 'u': 17,\n",
       " 'm': 11,\n",
       " 'n': 12,\n",
       " '.': 1,\n",
       " 'a': 3,\n",
       " '?': 2,\n",
       " 'l': 10}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# analyzer, tokenizer, token_pattern 등의 인수로 사용할 토큰 생성기를 선택\n",
    "vect = CountVectorizer(analyzer = \"char\").fit(corpus)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this': 2, 'the': 0, 'third': 1}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(token_pattern='t\\w+').fit(corpus)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'t ': 0, 't.': 3, 't  t.': 1, 't?': 4, 't  t?': 2}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# n-그램은 단어장 생성에 사용할 토큰의 크기 결정\n",
    "vect = CountVectorizer(ngram_range=(1,2), token_pattern='t\\W+').fit(corpus)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this': 3, 'the': 0, 'this the': 4, 'third': 2, 'the third': 1}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(ngram_range=(1,2),token_pattern='t\\w+').fit(corpus)\n",
    "vect.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF(Term Frequency – Inverse Document Frequency) 인코딩은 단어를 갯수 그대로 카운트하지 않고 모든 문서에 공통적으로 들어있는 단어의 경우 문서 구별 능력이 떨어진다고 보아 가중치를 축소\n",
    "문서 d(document)와 단어 t 에 대해 다음과 같이 계산\n",
    "\n",
    "tf-idf(d,t)=tf(d,t)⋅idf(t)\n",
    "\n",
    "tf(d,t): term frequency. 특정한 단어의 빈도수\n",
    "idf(t) : inverse document frequency. 특정한 단어가 들어 있는 문서의 수에 반비례하는 수\n",
    "n : 전체 문서의 수\n",
    "df(t) : 단어 t 를 가진 문서의 수\n",
    "idf(d,t)=log(n/(1+df(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\"\"\"\n",
    "The Corpus of Contemporary American English (COCA) is the only large, \n",
    "genre-balanced corpus of American English. COCA is probably the most \n",
    "widely-used corpus of English, and it is related to many other corpora of \n",
    "English that we have created, which offer unparalleled insight into variation \n",
    "in English.\n",
    "\"\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ant', 'document', 'second']\n",
      "\n",
      "{'document': 1, 'second': 2, 'ant': 0}\n",
      "\n",
      "[[0.         1.         0.        ]\n",
      " [0.         0.49084524 0.87124678]\n",
      " [1.         0.         0.        ]\n",
      " [0.         1.         0.        ]\n",
      " [0.         1.         0.        ]]\n",
      "[[0.         0.49084524 0.87124678]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidv = TfidfVectorizer(stop_words='english').fit(corpus)\n",
    "print(tfidv.get_feature_names())\n",
    "print()\n",
    "print(tfidv.vocabulary_)\n",
    "print()\n",
    "print(tfidv.transform(corpus).toarray())\n",
    "print(tfidv.transform(['This is the second document']).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "nlp10_Text Mining.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
